{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "colab_type": "code",
        "id": "Qz9_r5AusxjF",
        "outputId": "69a06db1-4f1c-4b11-c58a-d437d6d19eba"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from math import ceil\n",
        "import random\n",
        "from glob import glob\n",
        "\n",
        "import sys, os, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '4' \n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from .agent_environment import *\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cDBbS8astDPs"
      },
      "source": [
        "### Loading agent weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "colab_type": "code",
        "id": "wP07FmBCs57x",
        "outputId": "32e7fc46-db94-4c48-ed27-e1c1b48f4c32"
      },
      "outputs": [],
      "source": [
        "agent = DQN(input_shape=(30, 7), action_size=3, batch_size=128, UPDATE_TARGET=4,\n",
        "            DISCOUNT=0.9, DISCOUNT_DECAY=0, timestep=31, REPLAY_MEM_SZ=2000) \n",
        "\n",
        "model_path = os.path.join(os.path.realpath('.'), 'base_model')\n",
        "if os.path.exists(model_path) == False:\n",
        "    print('Trained model not found')\n",
        "\n",
        "agent.model = tf.keras.models.load_model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HTvIojNgtOGP"
      },
      "source": [
        "### Starting environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "AanU75hztLoc"
      },
      "outputs": [],
      "source": [
        "#1000 = 224 x 224\n",
        "#5000 = 500 x 500\n",
        "#10000 = 708 x 708\n",
        "#population_density = 0.02\n",
        "\n",
        "# Modify the seed_list to generate different random-environmental states\n",
        "env_actions = ['no-lockdown', 'semi-lockdown', 'lockdown', 'agent']\n",
        "envSetups = [{'height':577 , 'population':10000, 'seed':1, 'infected':70},\n",
        "            {'height':708 , 'population':10000, 'seed':6, 'infected':70},\n",
        "            {'height':1000 , 'population':10000, 'seed':4, 'infected':70}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "colab_type": "code",
        "id": "0BteSxdatNQ5",
        "outputId": "70e41b68-51f9-476b-e783-407bfa9844f0"
      },
      "outputs": [],
      "source": [
        "for envSetup in envSetups:\n",
        "    for act in env_actions:\n",
        "        print('Starting environ with the parameters', envSetup)\n",
        "        print('The actions are based on', act)\n",
        "\n",
        "        env = Env(height=envSetup['height'], width=envSetup['height'], \n",
        "                  population=envSetup['population'], \n",
        "                  infected_ratio=0.002, beta=np.inf, prob=1, boundary=0, \n",
        "                  cure_after=21, infect_after=2, death_distribution=[0.2, 0.8], \n",
        "                  day_step=15, day_limit=0, normalize=True, linear_reward=False)\n",
        "\n",
        "        done = False\n",
        "        tot_reward, actions, real_act = 0, 0, 0\n",
        "        st = time.time()\n",
        "        data = {'Day': [], 'R0': [], 'Active Cases': [], 'Infected': [], \n",
        "                'Economy': [], 'Death': [], 'Cured': [], \n",
        "                'Action': [], 'Population': env.population, 'R00': [], \n",
        "                'Reward': [], 'Action1': [], 'Action2': [], 'Action3': []} \n",
        "\n",
        "        agent.initReplay()\n",
        "\n",
        "        # Manually setting the number of infected population\n",
        "        env.first_infected = envSetup['infected']\n",
        "\n",
        "        current_state = np.array(list(env.reset(envSetup['seed']))+[0], dtype=np.float32)\n",
        "        cycle_count = 0\n",
        "        best_action = 0\n",
        "\n",
        "        while not done:\n",
        "            if act == 'agent':\n",
        "                reward_preds = agent.get_qs(current_state)\n",
        "                best_action = np.argmax(reward_preds)\n",
        "                data['Action1'].append(reward_preds[0, 0])\n",
        "                data['Action2'].append(reward_preds[0, 1])\n",
        "                data['Action3'].append(reward_preds[0, 2])\n",
        "            if act == 'lockdown':\n",
        "                best_action = 2\n",
        "            if act == 'semi-lockdown':\n",
        "                best_action = 1\n",
        "            if act == 'no-lockdown':\n",
        "                best_action = 0\n",
        "            \n",
        "            data['Day'].append(env.log['day'])\n",
        "            data['Active Cases'].append(env.log['active_cases'])\n",
        "            data['Infected'].append(env.log['infected'])\n",
        "            data['Cured'].append(env.log['cured'])\n",
        "            data['Economy'].append(env.log['economy'])\n",
        "            data['R0'].append(env.log['R0'])\n",
        "            data['R00'].append(env.log['R00'])\n",
        "            data['Death'].append(env.log['death'])\n",
        "            data['Action'].append(best_action)\n",
        "    \n",
        "            # Perform the action\n",
        "            new_state, reward, done, dct = env.step(best_action)\n",
        "            new_state = np.array(list(new_state)+[best_action], dtype=np.float32)\n",
        "            data['Reward'].append(reward)\n",
        "    \n",
        "            # Find the q values of the new state (after performing previous action)\n",
        "            tot_reward += reward\n",
        "    \n",
        "            agent.update_replay_memory((current_state, best_action, reward, new_state, done))\n",
        "            #agent.train(done, actions)\n",
        "            current_state = new_state\n",
        "            print(f\"\\rDay:{data['Day'][-1]}, R0:{data['R0'][-1]:0.1f}, Infected:{data['Infected'][-1]}, AC:{data['Active Cases'][-1]}\", end='')\n",
        "        \n",
        "        rl = len(data['Reward'])\n",
        "        data['RewardSum'] = [x for x in data['Reward']]\n",
        "        for i in range(rl-2, -1, -1):\n",
        "            data['RewardSum'][i] += data['Reward'][i+1]*agent.DISCOUNT\n",
        "        data['Total Infected'] = env.total_infected\n",
        "        data['Total Cured'] = env.total_cured\n",
        "        data['Total Death'] = env.total_death\n",
        "        data['Total Economy'] = np.sum(data['Economy'])\n",
        "        data['MAX_ECONOMY'] = env.observation_space_high[2]\n",
        "        data['TOTAL_POPULATION'] = env.observation_space_high[1]\n",
        "        \n",
        "        draw(data, rewards=False, plot=False, \n",
        "            #title=f\"Population: {env.population}, Initial Infectious: {infpop}\",\n",
        "            filename=os.path.join(os.path.realpath('.'), \n",
        "                                  \"dashboard_logs\", \n",
        "                                  f\"{env.height}_{env.population}_{envSetup['seed']}_{envSetup['infected']}_{act}\")\n",
        "            )\n",
        "        print('Log saved in dashboard_logs')\n",
        "        #actionvals.append(np.sum(data['Reward']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n9A40oOvtWCR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "dashboard.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
